# GPT-2-Model-Transformer-Block
This model is developed on the basis of LLM Birds View Architecture to generate words.
This model is not subjected for Training process.
<img width="1920" height="1200" alt="Screenshot (6)" src="https://github.com/user-attachments/assets/edbbf4ac-945b-4f39-bb4b-3eaee088f470" />
Model has 124 Million Parameters That are randomly initialized.
Vocabulary size is 50257 tokens using BPE Tokenizer used by GPT2 Model.
MultiHead Attention Mechanism used for this model
